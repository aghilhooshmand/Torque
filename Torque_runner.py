"""
Runner Module - Executes Python Code and Outputs Metrics to JSON

This module handles the execution and evaluation part:
- Option 1: Accept Torque DSL command → calls Torque_Mapper → executes → outputs metrics JSON
- Option 2: Read Python code from JSON file (generated by Torque_mapper.py) → executes → outputs metrics JSON
- Execute Python code to create sklearn estimator
- Run model on data
- Calculate all ML metrics
- Export results to JSON

Both Torque_Mapper.py and runner.py can be used separately:
- Torque_Mapper.py: Just mapping (DSL → AST → Python code → JSON)
- runner.py: Can work standalone (DSL → mapping → execution → metrics) OR read from existing JSON
"""

from typing import Dict, Any, Optional
import sys
import os
import json
import numpy as np
import pandas as pd
from datetime import datetime

# Add current directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from compiler import compile_ast_to_estimator
from Torque_mapper import TorqueMapper

# Scikit-learn imports
try:
    from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
    from sklearn.metrics import (
        accuracy_score,
        precision_score,
        recall_score,
        f1_score,
        roc_auc_score,
        confusion_matrix,
        classification_report,
        average_precision_score,
        cohen_kappa_score,
        matthews_corrcoef,
        log_loss,
    )
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False


class TorqueRunner:
    """
    Runs Torque DSL commands or Python code and evaluates models with comprehensive metrics.
    
    This class can work in two modes:
    1. Direct DSL execution: Accepts Torque command → calls Torque_Mapper → executes → outputs metrics
    2. JSON execution: Reads Python code from JSON file → executes → outputs metrics
    
    Both modes:
    - Execute Python code to create sklearn estimator
    - Run model on data
    - Calculate all ML metrics
    - Export results to JSON
    """
    
    def __init__(self):
        """Initialize the runner."""
        if not SKLEARN_AVAILABLE:
            raise ImportError(
                "scikit-learn is required for TorqueRunner. "
                "Install with: pip install scikit-learn"
            )
        # Initialize mapper for DSL to AST/Python code conversion
        self.mapper = TorqueMapper()
    
    def run_ast(
        self,
        ast: Dict,
        X: np.ndarray,
        y: np.ndarray,
        output_file: str = "Torque_runner_result.json",
        test_size: float = 0.3,
        cv_folds: int = 5,
        random_state: int = 42
    ) -> Dict[str, Any]:
        """
        Run AST on data and export all ML metrics to JSON.
        
        Note: This method is for direct AST execution. For the recommended workflow,
        use Torque_Mapper.export_to_json() to create a JSON file, then use run_from_json().
        
        Args:
            ast: AST dictionary (from Torque_mapper.dsl_to_ast())
            X: Feature matrix (n_samples, n_features)
            y: Target vector (n_samples,)
            output_file: Path to output JSON file (default: "Torque_runner_result.json")
            test_size: Proportion of data for testing (default: 0.3)
            cv_folds: Number of cross-validation folds (default: 5)
            random_state: Random seed for reproducibility (default: 42)
            
        Returns:
            Dictionary containing all evaluation metrics and metadata
            
        Example:
            >>> from Torque_mapper import TorqueMapper
            >>> from Torque_runner import TorqueRunner
            >>> 
            >>> mapper = TorqueMapper()
            >>> ast = mapper.dsl_to_ast('vote(LR(C=1.0), SVM())')
            >>> 
            >>> runner = TorqueRunner()
            >>> results = runner.run_ast(ast, X, y, output_file='results.json')
        """
        # Compile AST to sklearn estimator
        estimator = compile_ast_to_estimator(ast)
        
        return self._run_and_evaluate(
            estimator=estimator,
            X=X,
            y=y,
            output_file=output_file,
            test_size=test_size,
            cv_folds=cv_folds,
            random_state=random_state,
            source_info={"type": "ast", "ast": ast}
        )
    
    def run_dsl(
        self,
        torque_command: str,
        X: np.ndarray,
        y: np.ndarray,
        mapped_json_file: Optional[str] = None,
        metrics_json_file: str = "Torque_runner_result.json",
        test_size: float = 0.3,
        cv_folds: int = 5,
        random_state: int = 42
    ) -> Dict[str, Any]:
        """
        Accept Torque DSL command, convert to Python code, execute, and output metrics to JSON.
        
        This method:
        1. Calls Torque_Mapper.export_to_json() to convert DSL → AST → Python code → JSON
        2. Reads the generated JSON file
        3. Executes the Python code
        4. Runs model and calculates metrics
        5. Outputs metrics to another JSON file
        
        Args:
            torque_command: Torque DSL string (e.g., 'vote(LR(C=1.0), SVM())')
            X: Feature matrix (n_samples, n_features)
            y: Target vector (n_samples,)
            mapped_json_file: Path to save mapping JSON (default: auto-generated from command hash)
            metrics_json_file: Path to output JSON file for metrics (default: "Torque_runner_result.json")
            test_size: Proportion of data for testing (default: 0.3)
            cv_folds: Number of cross-validation folds (default: 5)
            random_state: Random seed for reproducibility (default: 42)
            
        Returns:
            Dictionary containing all evaluation metrics and metadata
            
        Example:
            >>> runner = TorqueRunner()
            >>> results = runner.run_dsl(
            ...     'vote(LR(C=1.0), SVM())',
            ...     X, y,
            ...     mapped_json_file='mapped_code.json',
            ...     metrics_json_file='metrics.json'
            ... )
        """
        import hashlib
        
        # Step 1: Generate mapping JSON file if not provided
        if mapped_json_file is None:
            # Auto-generate filename from command hash
            cmd_hash = hashlib.md5(torque_command.encode()).hexdigest()[:8]
            mapped_json_file = f"Torque_mapper_result_{cmd_hash}.json"
        
        print(f"Step 1: Converting Torque DSL to Python code...")
        print(f"  Command: {torque_command}")
        
        # Call Torque_Mapper to create mapping JSON
        mapping_result = self.mapper.export_to_json(
            torque_command=torque_command,
            output_file=mapped_json_file,
            variable_name="estimator"
        )
        
        print(f"  ✓ Mapping saved to: {mapped_json_file}")
        print()
        
        # Step 2: Run from the generated JSON file
        print(f"Step 2: Executing Python code and evaluating model...")
        results = self.run_from_json(
            json_file=mapped_json_file,
            X=X,
            y=y,
            output_file=metrics_json_file,
            test_size=test_size,
            cv_folds=cv_folds,
            random_state=random_state
        )
        
        # Add mapping file info to results
        results["mapping_file"] = mapped_json_file
        
        return results
    
    def run_from_json(
        self,
        json_file: str,
        X: np.ndarray,
        y: np.ndarray,
        output_file: str = "Torque_runner_result.json",
        test_size: float = 0.3,
        cv_folds: int = 5,
        random_state: int = 42
    ) -> Dict[str, Any]:
        """
        Read Python code from JSON file (generated by Torque_mapper.export_to_json),
        execute it, run model, and export all ML metrics to JSON.
        
        Use this method when you already have a mapping JSON file from Torque_mapper.
        
        Args:
            json_file: Path to JSON file containing Python code (from Torque_mapper.export_to_json)
            X: Feature matrix (n_samples, n_features)
            y: Target vector (n_samples,)
            output_file: Path to output JSON file for metrics (default: "torque_results.json")
            test_size: Proportion of data for testing (default: 0.3)
            cv_folds: Number of cross-validation folds (default: 5)
            random_state: Random seed for reproducibility (default: 42)
            
        Returns:
            Dictionary containing all evaluation metrics and metadata
            
        Example:
            >>> # First, generate JSON with Python code (optional - can use run_dsl instead)
            >>> from Torque_mapper import TorqueMapper
            >>> mapper = TorqueMapper()
            >>> mapper.export_to_json('vote(LR(C=1.0), SVM())', 'mapped.json')
            >>> 
            >>> # Then, run it
            >>> from Torque_runner import TorqueRunner
            >>> runner = TorqueRunner()
            >>> results = runner.run_from_json('mapped.json', X, y, 'results.json')
        """
        # Step 1: Read JSON file
        with open(json_file, 'r') as f:
            mapped_data = json.load(f)
        
        python_code = mapped_data.get("python_code")
        if not python_code:
            raise ValueError(f"JSON file {json_file} does not contain 'python_code' field")
        
        variable_name = mapped_data.get("variable_name", "estimator")
        torque_command = mapped_data.get("torque_command", "unknown")
        
        # Step 2: Execute Python code to create estimator
        # Create a local namespace for execution
        local_namespace = {}
        exec(python_code, {"__builtins__": __builtins__}, local_namespace)
        
        # Get the estimator from the namespace
        if variable_name not in local_namespace:
            raise ValueError(
                f"Python code did not create variable '{variable_name}'. "
                f"Make sure the code creates an estimator with this name."
            )
        
        estimator = local_namespace[variable_name]
        
        # Step 3: Run and evaluate
        return self._run_and_evaluate(
            estimator=estimator,
            X=X,
            y=y,
            output_file=output_file,
            test_size=test_size,
            cv_folds=cv_folds,
            random_state=random_state,
            source_info={
                "type": "json",
                "json_file": json_file,
                "torque_command": torque_command
            }
        )
    
    def _run_and_evaluate(
        self,
        estimator: Any,
        X: np.ndarray,
        y: np.ndarray,
        output_file: str,
        test_size: float,
        cv_folds: int,
        random_state: int,
        source_info: Dict
    ) -> Dict[str, Any]:
        """
        Internal method: Run estimator and evaluate with all metrics.
        
        Args:
            estimator: Sklearn estimator (already compiled)
            X: Feature matrix
            y: Target vector
            output_file: Path to output JSON file
            test_size: Proportion of data for testing
            cv_folds: Number of CV folds
            random_state: Random seed
            source_info: Information about the source (DSL or AST)
            
        Returns:
            Dictionary containing all evaluation metrics and metadata
        """
        print(f"Running estimator: {type(estimator).__name__}")
        print(f"Data shape: X={X.shape}, y={y.shape}")
        print()
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "source": source_info,
            "data_info": {
                "n_samples": int(X.shape[0]),
                "n_features": int(X.shape[1]),
                "n_classes": int(len(np.unique(y))),
                "class_distribution": {
                    int(k): int(v) 
                    for k, v in zip(*np.unique(y, return_counts=True))
                }
            },
            "parameters": {
                "test_size": test_size,
                "cv_folds": cv_folds,
                "random_state": random_state
            }
        }
        
        try:
            # Step 1: Store estimator info
            print("Step 1: Compiling estimator...")
            results["estimator_info"] = {
                "type": type(estimator).__name__,
                "module": type(estimator).__module__
            }
            print(f"  ✓ Created {type(estimator).__name__}")
            print()
            
            # Step 2: Split data
            print("Step 2: Splitting data...")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y,
                test_size=test_size,
                random_state=random_state,
                stratify=y
            )
            results["data_split"] = {
                "train_samples": int(X_train.shape[0]),
                "test_samples": int(X_test.shape[0]),
                "train_features": int(X_train.shape[1])
            }
            print(f"  ✓ Train: {X_train.shape[0]} samples, Test: {X_test.shape[0]} samples")
            print()
            
            # Step 3: Train the model
            print("Step 3: Training model...")
            estimator.fit(X_train, y_train)
            print("  ✓ Model trained")
            print()
            
            # Step 4: Make predictions
            print("Step 4: Making predictions...")
            y_pred = estimator.predict(X_test)
            y_pred_proba = None
            
            # Get prediction probabilities if available
            if hasattr(estimator, "predict_proba"):
                try:
                    y_pred_proba = estimator.predict_proba(X_test)
                    print("  ✓ Probability predictions available")
                except:
                    print("  ⚠ Probability predictions not available")
            else:
                print("  ⚠ Model does not support probability predictions")
            print()
            
            # Step 5: Calculate all metrics
            print("Step 5: Calculating metrics...")
            metrics = self._calculate_all_metrics(y_test, y_pred, y_pred_proba)
            results["metrics"] = metrics
            print("  ✓ All metrics calculated")
            print()
            
            # Step 6: Cross-validation
            print("Step 6: Running cross-validation...")
            cv_results = self._run_cross_validation(estimator, X, y, cv_folds, random_state)
            results["cross_validation"] = cv_results
            print("  ✓ Cross-validation completed")
            print()
            
            # Step 7: Confusion matrix
            print("Step 7: Generating confusion matrix...")
            cm = confusion_matrix(y_test, y_pred)
            results["confusion_matrix"] = {
                "matrix": cm.tolist(),
                "labels": [int(label) for label in np.unique(y)]
            }
            print("  ✓ Confusion matrix generated")
            print()
            
            # Step 8: Classification report
            print("Step 8: Generating classification report...")
            class_report = classification_report(y_test, y_pred, output_dict=True)
            results["classification_report"] = class_report
            print("  ✓ Classification report generated")
            print()
            
            results["status"] = "success"
            results["error"] = None
            
        except Exception as e:
            print(f"❌ Error: {str(e)}")
            import traceback
            results["status"] = "error"
            results["error"] = str(e)
            results["traceback"] = traceback.format_exc()
        
        # Save to JSON file
        print(f"Saving results to {output_file}...")
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"  ✓ Results saved to {output_file}")
        print()
        
        # Print summary
        if results["status"] == "success":
            print("=" * 70)
            print("SUMMARY")
            print("=" * 70)
            metrics = results["metrics"]
            print(f"Accuracy: {metrics['accuracy']:.4f}")
            print(f"Precision (macro): {metrics['precision_macro']:.4f}")
            print(f"Recall (macro): {metrics['recall_macro']:.4f}")
            print(f"F1-score (macro): {metrics['f1_macro']:.4f}")
            if "roc_auc" in metrics:
                print(f"ROC-AUC: {metrics['roc_auc']:.4f}")
            print()
        
        return results
    
    def _calculate_all_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_pred_proba: Optional[np.ndarray] = None
    ) -> Dict[str, float]:
        """
        Calculate comprehensive ML metrics.
        
        Args:
            y_true: True labels
            y_pred: Predicted labels
            y_pred_proba: Predicted probabilities (optional)
            
        Returns:
            Dictionary of all calculated metrics
        """
        metrics = {}
        
        # Basic classification metrics
        metrics["accuracy"] = float(accuracy_score(y_true, y_pred))
        
        # Precision, Recall, F1 (with different averaging)
        for average in ["macro", "micro", "weighted"]:
            metrics[f"precision_{average}"] = float(
                precision_score(y_true, y_pred, average=average, zero_division=0)
            )
            metrics[f"recall_{average}"] = float(
                recall_score(y_true, y_pred, average=average, zero_division=0)
            )
            metrics[f"f1_{average}"] = float(
                f1_score(y_true, y_pred, average=average, zero_division=0)
            )
        
        # Additional metrics
        metrics["cohen_kappa"] = float(cohen_kappa_score(y_true, y_pred))
        metrics["matthews_corrcoef"] = float(matthews_corrcoef(y_true, y_pred))
        
        # Probability-based metrics (if available)
        if y_pred_proba is not None:
            n_classes = len(np.unique(y_true))
            
            if n_classes == 2:
                # Binary classification
                metrics["roc_auc"] = float(roc_auc_score(y_true, y_pred_proba[:, 1]))
                metrics["average_precision"] = float(
                    average_precision_score(y_true, y_pred_proba[:, 1])
                )
                metrics["log_loss"] = float(log_loss(y_true, y_pred_proba))
            else:
                # Multi-class classification
                try:
                    metrics["roc_auc_ovr"] = float(
                        roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='macro')
                    )
                    metrics["roc_auc_ovo"] = float(
                        roc_auc_score(y_true, y_pred_proba, multi_class='ovo', average='macro')
                    )
                    metrics["log_loss"] = float(log_loss(y_true, y_pred_proba))
                except:
                    pass
        
        return metrics
    
    def _run_cross_validation(
        self,
        estimator: Any,
        X: np.ndarray,
        y: np.ndarray,
        cv_folds: int = 5,
        random_state: int = 42
    ) -> Dict[str, Any]:
        """
        Run cross-validation with multiple metrics.
        
        Args:
            estimator: Sklearn estimator
            X: Feature matrix
            y: Target vector
            cv_folds: Number of CV folds
            random_state: Random seed
            
        Returns:
            Dictionary of CV results
        """
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
        
        cv_results = {}
        
        # Metrics to evaluate
        scoring_metrics = {
            "accuracy": "accuracy",
            "precision_macro": "precision_macro",
            "recall_macro": "recall_macro",
            "f1_macro": "f1_macro",
        }
        
        for metric_name, scoring in scoring_metrics.items():
            try:
                scores = cross_val_score(
                    estimator, X, y, cv=cv, scoring=scoring, n_jobs=-1
                )
                cv_results[metric_name] = {
                    "mean": float(scores.mean()),
                    "std": float(scores.std()),
                    "scores": scores.tolist(),
                    "min": float(scores.min()),
                    "max": float(scores.max())
                }
            except:
                pass
        
        return cv_results
    
    @staticmethod
    def load_config(config_file: str) -> Dict[str, Any]:
        """
        Load configuration from JSON file.
        
        Args:
            config_file: Path to JSON config file
            
        Returns:
            Dictionary containing configuration
            
        Example config file structure:
        {
          "data": {
            "file": "data/processed.cleveland.csv",
            "target_column": "class",
            "delimiter": ",",
            "header": true
          },
          "splitting": {
            "test_size": 0.3,
            "random_state": 42,
            "stratify": true
          },
          "evaluation": {
            "cv_folds": 5,
            "random_state": 42
          },
          "output": {
            "mapped_json_file": "Torque_mapper_result.json",
            "metrics_json_file": "Torque_runner_result.json"
          }
        }
        """
        with open(config_file, 'r') as f:
            config = json.load(f)
        return config
    
    @staticmethod
    def load_data_from_config(config: Dict[str, Any]) -> tuple:
        """
        Load data from file specified in config.
        
        Args:
            config: Configuration dictionary with 'data' section
            
        Returns:
            Tuple of (X, y) where X is feature matrix and y is target vector
        """
        data_config = config.get("data", {})
        data_file = data_config.get("file")
        
        if not data_file:
            raise ValueError("Config file must specify 'data.file'")
        
        if not os.path.exists(data_file):
            raise FileNotFoundError(f"Data file not found: {data_file}")
        
        # Load CSV file
        delimiter = data_config.get("delimiter", ",")
        header = 0 if data_config.get("header", True) else None
        
        df = pd.read_csv(data_file, delimiter=delimiter, header=header)
        
        # Get target column
        target_column = data_config.get("target_column")
        if not target_column:
            # Assume last column is target
            target_column = df.columns[-1]
            print(f"⚠ No target_column specified, using last column: {target_column}")
        
        if target_column not in df.columns:
            raise ValueError(f"Target column '{target_column}' not found in data. Available columns: {list(df.columns)}")
        
        # Split features and target
        X = df.drop(columns=[target_column]).values
        y = df[target_column].values
        
        print(f"✓ Loaded data from: {data_file}")
        print(f"  Features: {X.shape[1]}, Samples: {X.shape[0]}")
        print(f"  Target column: {target_column}")
        print(f"  Classes: {len(np.unique(y))}")
        
        return X, y


# Convenience functions
def run_dsl(
    torque_command: str,
    X: np.ndarray,
    y: np.ndarray,
    mapped_json_file: Optional[str] = None,
    metrics_json_file: str = "torque_results.json",
    **kwargs
) -> Dict[str, Any]:
    """
    Convenience function: Run Torque DSL command (maps, executes, and outputs metrics).
    
    Args:
        torque_command: Torque DSL string
        X: Feature matrix
        y: Target vector
        mapped_json_file: Path to save mapping JSON (optional, auto-generated if None)
        metrics_json_file: Path to output JSON file for metrics
        **kwargs: Additional arguments for run_dsl()
        
    Returns:
        Dictionary containing all evaluation metrics
    """
    runner = TorqueRunner()
    return runner.run_dsl(
        torque_command, X, y,
        mapped_json_file=mapped_json_file,
        metrics_json_file=metrics_json_file,
        **kwargs
    )


def run_from_json(
    json_file: str,
    X: np.ndarray,
    y: np.ndarray,
    output_file: str = "torque_results.json",
    **kwargs
) -> Dict[str, Any]:
    """
    Convenience function: Read Python code from JSON and run it.
    
    Args:
        json_file: Path to JSON file from Torque_mapper.export_to_json()
        X: Feature matrix
        y: Target vector
        output_file: Path to output JSON file for metrics
        **kwargs: Additional arguments for run_from_json()
        
    Returns:
        Dictionary containing all evaluation metrics
    """
    runner = TorqueRunner()
    return runner.run_from_json(json_file, X, y, output_file, **kwargs)


# Command-line interface
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Torque Runner - Execute Torque DSL commands and evaluate models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
All settings are normally read from a config file.

Examples:
  # Recommended: use config file
  python Torque_runner.py --config config/Torque_runner_config.json
  
  # (Advanced) Legacy: run with direct arguments is not supported anymore,
  # everything should be in the config file for reproducibility.
        """
    )
    
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Path to JSON config file containing all settings"
    )
    
    args = parser.parse_args()
    
    # Validate config file exists
    if not os.path.exists(args.config):
        print(f"❌ Error: Config file not found: {args.config}")
        sys.exit(1)
    
    # Load config
    config = TorqueRunner.load_config(args.config)
    print(f"✓ Loaded config from: {args.config}")
    
    # Validate required sections
    if "data" not in config:
        print("❌ Error: Config file must contain 'data' section")
        sys.exit(1)
    
    # Load data from config
    X, y = TorqueRunner.load_data_from_config(config)
    
    # Get evaluation parameters from config
    splitting_config = config.get("splitting", {})
    test_size = splitting_config.get("test_size", 0.3)
    random_state = splitting_config.get("random_state", 42)
    
    evaluation_config = config.get("evaluation", {})
    cv_folds = evaluation_config.get("cv_folds", 5)
    eval_random_state = evaluation_config.get("random_state", random_state)
    
    # Get output file paths from config
    output_config = config.get("output", {})
    metrics_output = output_config.get("metrics_json_file", "Torque_runner_result.json")
    mapped_output = output_config.get("mapped_json_file")
    
    # Create runner
    runner = TorqueRunner()
    
    # Determine input type: torque_command or json_file
    torque_command = config.get("torque_command")
    json_file = config.get("json_file")
    
    if torque_command:
        # Run from Torque command
        print(f"\n{'='*70}")
        print("Running Torque DSL Command")
        print(f"{'='*70}")
        print(f"Command: {torque_command}")
        print()
        
        results = runner.run_dsl(
            torque_command=torque_command,
            X=X,
            y=y,
            mapped_json_file=mapped_output,
            metrics_json_file=metrics_output,
            test_size=test_size,
            cv_folds=cv_folds,
            random_state=eval_random_state
        )
    elif json_file:
        # Run from JSON file
        if not os.path.exists(json_file):
            print(f"❌ Error: JSON file not found: {json_file}")
            sys.exit(1)
        
        print(f"\n{'='*70}")
        print("Running from Mapping JSON File")
        print(f"{'='*70}")
        print(f"JSON file: {json_file}")
        print()
        
        results = runner.run_from_json(
            json_file=json_file,
            X=X,
            y=y,
            output_file=metrics_output,
            test_size=test_size,
            cv_folds=cv_folds,
            random_state=eval_random_state
        )
    else:
        print("❌ Error: Config file must contain either 'torque_command' or 'json_file'")
        sys.exit(1)
    
    # Print summary
    if results.get("status") == "success":
        print(f"\n{'='*70}")
        print("SUMMARY")
        print(f"{'='*70}")
        metrics = results.get("metrics", {})
        print(f"Accuracy: {metrics.get('accuracy', 0):.4f}")
        print(f"Precision (macro): {metrics.get('precision_macro', 0):.4f}")
        print(f"Recall (macro): {metrics.get('recall_macro', 0):.4f}")
        print(f"F1-score (macro): {metrics.get('f1_macro', 0):.4f}")
        if "roc_auc" in metrics:
            print(f"ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"\n✓ Results saved to: {metrics_output}")
    else:
        print(f"\n❌ Error occurred: {results.get('error', 'Unknown error')}")
        sys.exit(1)


def run_ast(
    ast: Dict,
    X: np.ndarray,
    y: np.ndarray,
    output_file: str = "torque_results.json",
    **kwargs
) -> Dict[str, Any]:
    """
    Convenience function: Run AST on data and export results to JSON.
    
    Args:
        ast: AST dictionary
        X: Feature matrix
        y: Target vector
        output_file: Path to output JSON file
        **kwargs: Additional arguments for run_ast()
        
    Returns:
        Results dictionary
    """
    runner = TorqueRunner()
    return runner.run_ast(ast, X, y, output_file=output_file, **kwargs)


